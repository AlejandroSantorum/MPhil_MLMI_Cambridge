{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using sentiment lexicon  ---\n",
      "token-only results: 0.67\n",
      "magnitude results: 0.68\n",
      "magnitude lexicon results are not significant with respect to token-only\n"
     ]
    }
   ],
   "source": [
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "# use sign test for all significance testing\n",
    "signTest=SignTest()\n",
    "\n",
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()\n",
    "\n",
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "threshold=8\n",
    "\n",
    "# question 0.1\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=False)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "lexicon.classify(corpus.reviews,threshold,magnitude=True)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.2f}\")\n",
    "\n",
    "# question 0.2\n",
    "p_value=signTest.getSignificance(token_preds,magnitude_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"magnitude lexicon results are {significance} with respect to token-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using Naive Bayes on held-out test set ---\n",
      "Accuracy without smoothing: 0.46\n"
     ]
    }
   ],
   "source": [
    "# question 1.0\n",
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "# store predictions from classifier\n",
    "non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy without smoothing: {NB.getAccuracy():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using smoothing: 0.82\n",
      "results using smoothing are significant with respect to no smoothing\n"
     ]
    }
   ],
   "source": [
    "# question 2.0\n",
    "# use smoothing\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus.train)\n",
    "NB.test(corpus.test)\n",
    "smoothed_preds=NB.predictions\n",
    "# saving this for use later\n",
    "num_non_stemmed_features=len(NB.vocabulary)\n",
    "print(f\"Accuracy using smoothing: {NB.getAccuracy():.2f}\")\n",
    "\n",
    "\n",
    "# question 2.1\n",
    "# see if smoothing significantly improves results\n",
    "p_value=signTest.getSignificance(non_smoothed_preds,smoothed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing are {significance} with respect to no smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using 10-fold cross-evaluation ---\n",
      "Accuracy: 0.810\n",
      "Std. Dev: 0.020736441353327688\n"
     ]
    }
   ],
   "source": [
    "# question 3.0\n",
    "print(\"--- classifying reviews using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus)\n",
    "# using cross-eval for smoothed predictions from now on\n",
    "smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stemming corpus ---\n",
      "--- cross-validating NB using stemming ---\n",
      "Accuracy: 0.814\n",
      "Std. Dev: 0.026\n",
      "results using smoothing are not significant with respect to no smoothing\n",
      "--- determining the number of features before/after stemming ---\n",
      "Number of features before stemming: 52555\n",
      "Number of features after stemming: 32404\n"
     ]
    }
   ],
   "source": [
    "# question 4.0\n",
    "print(\"--- stemming corpus ---\")\n",
    "# retrieve corpus with tokenized text and stemming (using porter)\n",
    "stemmed_corpus=MovieReviewCorpus(stemming=True,pos=False)\n",
    "print(\"--- cross-validating NB using stemming ---\")\n",
    "NB.crossValidate(stemmed_corpus)\n",
    "stemmed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.3f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.3f}\")\n",
    "\n",
    "# TODO Q4.1\n",
    "# see if stemming significantly improves results on smoothed NB\n",
    "p_value=signTest.getSignificance(stemmed_preds,smoothed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing are {significance} with respect to no smoothing\")\n",
    "\n",
    "# TODO Q4.2\n",
    "print(\"--- determining the number of features before/after stemming ---\")\n",
    "print(\"Number of features before stemming:\", num_non_stemmed_features) #variable set previously\n",
    "NB.train(stemmed_corpus.train)\n",
    "print(\"Number of features after stemming:\", len(NB.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- cross-validating naive bayes using smoothing and bigrams ---\n",
      "Accuracy: 0.76\n",
      "Std. Dev: 0.02\n",
      "results using smoothing and bigrams are significant with respect to smoothing only\n",
      "--- cross-validating naive bayes using smoothing and trigrams ---\n",
      "Accuracy: 0.70\n",
      "Std. Dev: 0.03\n",
      "results using smoothing and trigrams are significant with respect to smoothing only\n",
      "--- determining the number of features before/after using bigrams ---\n",
      "Number of features before using bigrams (Q3): 52555\n",
      "Number of features after using bigrams: 500086\n",
      "Number of features after using trigrams: 1015074\n"
     ]
    }
   ],
   "source": [
    "# question Q5.0\n",
    "# cross-validate model using smoothing and bigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and bigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.crossValidate(corpus)\n",
    "smoothed_and_bigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.2f}\")\n",
    "# counting features using bigrams\n",
    "NB.train(corpus.train)\n",
    "num_bigram_features = len(NB.vocabulary)\n",
    "\n",
    "# see if bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(smoothed_preds,smoothed_and_bigram_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams are {signifance} with respect to smoothing only\")\n",
    "\n",
    "\n",
    "print(\"--- cross-validating naive bayes using smoothing and trigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=True,discard_closed_class=False)\n",
    "NB.crossValidate(corpus)\n",
    "smoothed_and_trigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.2f}\")\n",
    "# counting features using trigrams\n",
    "NB.train(corpus.train)\n",
    "num_trigram_features = len(NB.vocabulary)\n",
    "\n",
    "# see if bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(smoothed_preds,smoothed_and_trigram_preds)\n",
    "signifance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and trigrams are {signifance} with respect to smoothing only\")\n",
    "\n",
    "\n",
    "\n",
    "# TODO Q5.1\n",
    "print(\"--- determining the number of features before/after using bigrams ---\")\n",
    "print(\"Number of features before using bigrams (Q3):\", num_non_stemmed_features) #variable set previously\n",
    "print(\"Number of features after using bigrams:\", num_bigram_features)\n",
    "print(\"Number of features after using trigrams:\", num_trigram_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using SVM 10-fold cross-eval ---\n",
      "Accuracy: 0.8380\n",
      "Std. Dev: 0.020\n"
     ]
    }
   ],
   "source": [
    "# TODO Q6 and 6.1\n",
    "\n",
    "# retrieve corpus\n",
    "corpus=MovieReviewCorpus(stemming=False,pos=False)\n",
    "\n",
    "print(\"--- classifying reviews using SVM 10-fold cross-eval ---\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(corpus)\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.4f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- adding in POS information to corpus ---\n",
      "--- training svm on word+pos features ----\n",
      "Accuracy: 0.8345\n",
      "Std. Dev: 0.026\n",
      "--- training svm discarding closed-class words ---\n",
      "Accuracy: 0.8360\n",
      "Std. Dev: 0.023\n"
     ]
    }
   ],
   "source": [
    "# TODO Q7\n",
    "\n",
    "print(\"--- adding in POS information to corpus ---\")\n",
    "# retrieve corpus with POS information\n",
    "corpus_pos = MovieReviewCorpus(stemming=False,pos=True)\n",
    "\n",
    "print(\"--- training svm on word+pos features ----\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "SVM.crossValidate(corpus_pos)\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.4f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.3f}\")\n",
    "\n",
    "print(\"--- training svm discarding closed-class words ---\")\n",
    "SVM_dcc=SVMText(bigrams=False,trigrams=False,discard_closed_class=True)\n",
    "SVM_dcc.crossValidate(corpus_pos)\n",
    "print(f\"Accuracy: {SVM_dcc.getAccuracy():.4f}\") \n",
    "print(f\"Std. Dev: {SVM_dcc.getStdDeviation():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (40% of the marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- reading dataset from keras.datasets ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- getting vocabulary ---\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "print(\"--- reading dataset from keras.datasets ---\")\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(train_reviews, train_labels), (test_reviews, test_labels) = imdb.load_data()\n",
    "\n",
    "print(\"--- getting vocabulary ---\")\n",
    "vocab_word_to_id = imdb.get_word_index()\n",
    "vocab_word_to_id = {word:(word_id + 3) for word, word_id in vocab_word_to_id.items()}\n",
    "vocab_word_to_id[\"<PAD>\"] = 0\n",
    "vocab_word_to_id[\"<START>\"] = 1\n",
    "vocab_word_to_id[\"<UNK>\"] = 2\n",
    "vocab_word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "vocab_id_to_word =  dict([(value, key) for (key, value) in vocab_word_to_id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int_review(int_review):\n",
    "    return [vocab_id_to_word.get(id, \"?\") for id in int_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- concatenating train and test reviews to infer their embeddings at once ---\n",
      "--- creating documents after decoding reviews ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- concatenating train and test reviews to infer their embeddings at once ---\")\n",
    "reviews = np.concatenate((train_reviews, test_reviews))\n",
    "\n",
    "print(\"--- creating documents after decoding reviews ---\")\n",
    "docs = [TaggedDocument(decode_int_review(int_review), [i]) for i, int_review in enumerate(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pre-training doc2vec model ---\n"
     ]
    }
   ],
   "source": [
    "# question 8.0\n",
    "print(\"--- pre-training doc2vec model ---\")\n",
    "d2v_model = Doc2Vec(docs, dm=0, min_count=2, vector_size=100, hs=0, negative=5, epochs=100, sample=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.save(\"d2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- extracting learned embeddings and splitting them into training and testing sets\n",
      "training shape: (25000, 100)\n",
      "testing shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- extracting learned embeddings and splitting them into training and testing sets\")\n",
    "embeddings = d2v_model.dv.get_normed_vectors()\n",
    "train_embeddings, test_embeddings = np.split(embeddings, [25000]) # 25K because we have 25K reviews for training and 25K for testing\n",
    "\n",
    "print(\"training shape:\", train_embeddings.shape)\n",
    "print(\"testing shape:\", test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- using document embeddings ---\n",
      "Accuracy: 0.90\n",
      "Std. Dev: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"--- using document embeddings ---\")\n",
    "\n",
    "SVMD2V = SVMDoc2Vec(None)\n",
    "SVMD2V.train(train_embeddings, train_labels)\n",
    "SVMD2V.test(test_embeddings, test_labels)\n",
    "\n",
    "print(f\"Accuracy: {SVMD2V.getAccuracy():.2f}\") \n",
    "print(f\"Std. Dev: {SVMD2V.getStdDeviation():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
